{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52503a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utilis.network_analysis as na\n",
    "import src.data.dataDario as dl\n",
    "import src.utilis.utilisDario as ut\n",
    "import src.plotting as pl \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt # Import for displaying plots\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e30c8",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "First, we load all the necessary data files using our custom processing function. This function handles reading the CSVs, cleaning subreddit names, and filtering the datasets based on the list of approved subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960585b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_FILE = \"data/subreddit_matches_approved.csv\"   \n",
    "EMBEDDINGS_FILE = \"data/web-redditEmbeddings-subreddits.csv\" \n",
    "TITLE_FILE = \"data/soc-redditHyperlinks-title.tsv\"\n",
    "BODY_FILE = \"data/soc-redditHyperlinks-body.tsv\"\n",
    "APPROVED_MAPPING_FILE = \"data/subreddit_matches_approved.csv\"\n",
    "COUNTRY_EXPANDED = \"data/df_country_expanded.csv\"\n",
    "MAP_COUNTRY_EXPANDED = \"data/country_matches_map_exp.csv\"\n",
    "\n",
    "df_countries=dl.load_country_subreddits(COUNTRY_FILE)\n",
    "df_embeddings=dl.load_embeddings(EMBEDDINGS_FILE)\n",
    "df_embeddings_countries= dl.filter_embeddings_by_country(df_embeddings, df_countries)\n",
    "df_posts=dl.load_post_data(TITLE_FILE, BODY_FILE)\n",
    "df_post_with_1_country, df_post_between_countries=dl.filter_posts_by_country(df_posts, df_countries)\n",
    "df_country_exp = pd.read_csv(COUNTRY_EXPANDED)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_countries)} approved country-related subreddits.\")\n",
    "print(f\"Loaded {len(df_embeddings)} embeddings.\")\n",
    "print(f\"Loaded {len(df_embeddings_countries)} embeddings representing country subreddits.\")\n",
    "print(f\"Loaded {len(df_post_with_1_country)} posts with at least one approved country-related subreddit.\")\n",
    "print(f\"Loaded {len(df_post_between_countries)} posts between approved country-related subreddits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32ce9c",
   "metadata": {},
   "source": [
    "Notice that the embedding file for the countries has only 1912 rows while the approved subreddits are more than 3600. It's because a lot of country-related subreddits don't appear in the embedding file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c74add",
   "metadata": {},
   "source": [
    "# Familiarizing ourselves with the main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e16e4",
   "metadata": {},
   "source": [
    "### Description of variables in the dataset\n",
    "\n",
    "SOURCE_SUBREDDIT: the subreddit where the link originates\n",
    "\n",
    "TARGET_SUBREDDIT: the subreddit where the link ends\n",
    "\n",
    "POST_ID: the post in the source subreddit that starts the link\n",
    "\n",
    "TIMESTAMP: time time of the post\n",
    "\n",
    "POST_LABEL: label indicating if the source post is explicitly negative towards the target post. \n",
    "\n",
    "The value is -1 if the source is negative towards the target, and 1 if it is neutral or positive. \n",
    "\n",
    "The label is created using crowd-sourcing and training a text based classifier, and is better than simple sentiment analysis of the posts. Please see the reference paper for details.\n",
    "\n",
    "POST_PROPERTIES: a vector representing the text properties of the source post, listed as a list of comma separated numbers. The vector elements are the following:\n",
    "\n",
    "01. Number of characters\n",
    "2. Number of characters without counting white space\n",
    "3. Fraction of alphabetical characters\n",
    "4. Fraction of digits\n",
    "5. Fraction of uppercase characters\n",
    "6. Fraction of white spaces\n",
    "7. Fraction of special characters, such as comma, exclamation mark, etc.\n",
    "8. Number of words\n",
    "9. Number of unique works\n",
    "10. Number of long words (at least 6 characters)\n",
    "11. Average word length\n",
    "12. Number of unique stopwords\n",
    "13. Fraction of stopwords\n",
    "14. Number of sentences\n",
    "15. Number of long sentences (at least 10 words)\n",
    "16. Average number of characters per sentence\n",
    "17. Average number of words per sentence\n",
    "18. Automated readability index\n",
    "19. Positive sentiment calculated by VADER\n",
    "20. Negative sentiment calculated by VADER\n",
    "21. Compound sentiment calculated by VADER\n",
    "22. LIWC_Funct\n",
    "23. LIWC_Pronoun\n",
    "24. LIWC_Ppron\n",
    "25. LIWC_I\n",
    "26. LIWC_We\n",
    "27. LIWC_You\n",
    "28. LIWC_SheHe\n",
    "29. LIWC_They\n",
    "30. LIWC_Ipron\n",
    "31. LIWC_Article\n",
    "32. LIWC_Verbs\n",
    "33. LIWC_AuxVb\n",
    "34. LIWC_Past\n",
    "35. LIWC_Present\n",
    "36. LIWC_Future\n",
    "37. LIWC_Adverbs\n",
    "38. LIWC_Prep\n",
    "39. LIWC_Conj\n",
    "40. LIWC_Negate\n",
    "41. LIWC_Quant\n",
    "42. LIWC_Numbers\n",
    "43. LIWC_Swear\n",
    "44. LIWC_Social\n",
    "45. LIWC_Family\n",
    "46. LIWC_Friends\n",
    "47. LIWC_Humans\n",
    "48. LIWC_Affect\n",
    "49. LIWC_Posemo\n",
    "50. LIWC_Negemo\n",
    "51. LIWC_Anx\n",
    "52. LIWC_Anger\n",
    "53. LIWC_Sad\n",
    "54. LIWC_CogMech\n",
    "55. LIWC_Insight\n",
    "56. LIWC_Cause\n",
    "57. LIWC_Discrep\n",
    "58. LIWC_Tentat\n",
    "59. LIWC_Certain\n",
    "60. LIWC_Inhib\n",
    "61. LIWC_Incl\n",
    "62. LIWC_Excl\n",
    "63. LIWC_Percept\n",
    "64. LIWC_See\n",
    "65. LIWC_Hear\n",
    "66. LIWC_Feel\n",
    "67. LIWC_Bio\n",
    "68. LIWC_Body\n",
    "69. LIWC_Health\n",
    "70. LIWC_Sexual\n",
    "71. LIWC_Ingest\n",
    "72. LIWC_Relativ\n",
    "73. LIWC_Motion\n",
    "74. LIWC_Space\n",
    "75. LIWC_Time\n",
    "76. LIWC_Work\n",
    "77. LIWC_Achiev\n",
    "78. LIWC_Leisure\n",
    "79. LIWC_Home\n",
    "80. LIWC_Money\n",
    "81. LIWC_Relig\n",
    "82. LIWC_Death\n",
    "83. LIWC_Assent\n",
    "84. LIWC_Dissent\n",
    "85. LIWC_Nonflu\n",
    "86. LIWC_Filler\n",
    "\n",
    "LIWC - linguistic inquiry and word count (codebook: https://www.liwc.app/static/documents/LIWC-22%20Manual%20-%20Development%20and%20Psychometrics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf08e86",
   "metadata": {},
   "source": [
    "### 1. Look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58decc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea96788",
   "metadata": {},
   "source": [
    "We can see that the DataFrame of all the posts contains the source and target of each post as well as several properties related to the posts themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37106c",
   "metadata": {},
   "source": [
    "### 2. Look at the total number of unique subreddits and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5431cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both source and target columns into a single Pandas Series\n",
    "all_subreddits_series = pd.concat([df_posts['SOURCE_SUBREDDIT'], df_posts['TARGET_SUBREDDIT']])\n",
    "\n",
    "# Get the unique values from this combined series and convert to a list\n",
    "unique_subreddit_list = all_subreddits_series.unique().tolist()\n",
    "print(f\"Found {len(unique_subreddit_list)} unique subreddits.\")\n",
    "\n",
    "# Display the total number of posts in the dataset\n",
    "total_posts = len(df_posts)\n",
    "print(f\"Found {total_posts} posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907e9b9",
   "metadata": {},
   "source": [
    "### 3.  Expand properties column\n",
    "Here we create a separate column for each value in the properties column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4646da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split POST_PROPERTIES into columns\n",
    "df_posts[ut.post_props_cols] = df_posts[\"PROPERTIES\"].str.split(\",\", expand=True).astype(float)\n",
    "df_combined = df_posts.drop(columns=[\"PROPERTIES\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1529d1",
   "metadata": {},
   "source": [
    "### 4. Study the average properties of the different subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ea484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get subreddit counts\n",
    "subreddit_counts = df_combined[\"SOURCE_SUBREDDIT\"].value_counts()\n",
    "\n",
    "# Keep only subreddits with at least 20 posts\n",
    "valid_subreddits = subreddit_counts[subreddit_counts >= 20].index\n",
    "\n",
    "# Now compute averages for those\n",
    "avg_props_by_subreddit = (\n",
    "    df_combined[df_combined[\"SOURCE_SUBREDDIT\"].isin(valid_subreddits)]\n",
    "    .groupby(\"SOURCE_SUBREDDIT\")[ut.post_props_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "display(avg_props_by_subreddit.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cc362",
   "metadata": {},
   "source": [
    "Here we can look at the top 5 subreddits for any of the properties. We can for example look at the 5 subreddits with the highest money related sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345729a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_subreddits = {}\n",
    "\n",
    "for col in ut.post_props_cols:\n",
    "    # For each metric, take the top 5 subreddits\n",
    "    top5_subreddits[col] = (\n",
    "        avg_props_by_subreddit[[col]]\n",
    "        .sort_values(by=col, ascending=False)\n",
    "        .head(5)\n",
    "    )\n",
    "\n",
    "# Display the top 5 subreddits for sentiment negative as an example\n",
    "print(\"Top 5 subreddits by avg money related sentiment:\\n\")\n",
    "display(top5_subreddits[\"LIWC_Money\"]) # If you want to see another metric, change the column name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75b23c",
   "metadata": {},
   "source": [
    "# Mapping Subreddits to Countries\n",
    "To understand country-specific interactions, we first needed to identify which subreddits belong to which countries. This notebook uses two different sets of country mappings at different stages:\n",
    "\n",
    "1. The Approved List (Used First)\n",
    "Generation: An initial automated mapping was created (in make_initial_subreddit_maps.ipynb and filter_matches.ipynb). This found obvious matches (e.g., brasilonreddit -> Brazil) but also incorrect ones (e.g., askinsurance -> France).\n",
    "\n",
    "Verification: This initial list was manually reviewed, and each match was either \"approved\" or \"rejected\".\n",
    "\n",
    "Usage: For the first part of our analysis, we use only this manually approved list to ensure high accuracy.\n",
    "\n",
    "2. The Expanded List (Used Later)\n",
    "Generation: Later in this notebook, we introduce a new, much larger list of subreddit-to-country mappings. This list also includes matches made based on states and cities and linkes them to the relevant country. \n",
    "\n",
    "Verification: This expanded list is a raw, automated output and has not been manually approved yet. (We plan to verify this list at a later time).\n",
    "\n",
    "Usage: We use this unapproved list for the second part of our analysis to explore broader trends with a larger dataset, accepting that it contains unverified data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902d72b",
   "metadata": {},
   "source": [
    "### 1. Map subreddits to the country they belong to and look at their average properties\n",
    "Here we can look at which countries have the highest and lowest values for certain properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60392d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map posts to countries using the approved mapping\n",
    "posts_with_countries = ut.map_posts_to_countries(df_combined, APPROVED_MAPPING_FILE)\n",
    "country_posts_df = posts_with_countries.dropna(subset=['country'])\n",
    "\n",
    "# First, get post counts per country \n",
    "# We use 'country_posts_df' which has NaN countries removed\n",
    "country_post_counts = country_posts_df[\"country\"].value_counts()\n",
    "\n",
    "# Keep only countries with at least 20 posts \n",
    "min_posts_threshold = 20\n",
    "valid_countries = country_post_counts[country_post_counts >= min_posts_threshold].index\n",
    "\n",
    "print(f\"Found {len(valid_countries)} countries with at least {min_posts_threshold} posts.\")\n",
    "\n",
    "# Now compute averages for *those* countries\n",
    "# We filter the DataFrame first, then groupby\n",
    "avg_props_by_country = (\n",
    "    country_posts_df[country_posts_df[\"country\"].isin(valid_countries)]\n",
    "    .groupby(\"country\")[ut.post_props_cols] \n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_countries = {}\n",
    "low5_countries = {}\n",
    "\n",
    "for col in ut.post_props_cols:\n",
    "    # For each metric, take the top 5 countries\n",
    "    top5_countries[col] = (\n",
    "        avg_props_by_country[[col]]\n",
    "        .sort_values(by=col, ascending=False)\n",
    "        .head(5)\n",
    "    )\n",
    "    low5_countries[col] = (\n",
    "        avg_props_by_country[[col]]\n",
    "        .sort_values(by=col, ascending=True)\n",
    "        .head(5)\n",
    "    )  \n",
    "\n",
    "# Display the top 5 countries for sentiment negative as an example\n",
    "print(\"Top 5 countries by avg religion related sentiment:\\n\")\n",
    "display(top5_countries[\"LIWC_Relig\"]) # If you want to see another metric, change the column name here\n",
    "\n",
    "print(\"Bottom 5 countries by avg religion related sentiment:\\n\")\n",
    "display(low5_countries[\"LIWC_Relig\"]) # Change the column name here as well if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f69c69",
   "metadata": {},
   "source": [
    "Above, we look at the 5 countries that had the highest and lowest average score for religious sentiments in the posts from the subreddits mapped to spesific countries. \n",
    "\n",
    "Top 5: The high scores for Sri Lanka, Israel, Saudi Arabia, Bangladesh, and Mongolia are expected, as religion is central to national identity and daily life in these countries.\n",
    "\n",
    "Bottom 5: The low scores for Albania, Bulgaria, Estonia, and Hong Kong are also plausible. These are regions known for high rates of secularism or post-communist atheism.\n",
    "\n",
    "Potential Outlier: The low score for Jamaica is the only surprise, as it's generally considered a very religious country. This anomaly might be due to data limitations, such as a small or biased sample of mapped subreddits.\n",
    "\n",
    "Overall, the findings strongly mirror global trends, distinguishing between highly religious and highly secular nations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffa262",
   "metadata": {},
   "source": [
    "### 2. Country interactions\n",
    "We look at all posts that have the source and target as a country and then study the number of interactions between these countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63456b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process using the APPROVED list \n",
    "approved_country_interactions, merged_approved = ut.process_country_interactions(\n",
    "    df_combined, \n",
    "    APPROVED_MAPPING_FILE, \n",
    "    remove_self_loops=True\n",
    ")\n",
    "\n",
    "print(f\"--- Approved Country Interactions ({len(approved_country_interactions)} rows) ---\")\n",
    "display(approved_country_interactions.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fca5af",
   "metadata": {},
   "source": [
    "Based on these results, the mappings appear to be highly relevant and effective.\n",
    "\n",
    "The top-ranking interactions are not random. They clearly reflect significant real-world relationships:\n",
    "\n",
    "Geographic & Cultural Neighbors: The high volume of interactions between the United Kingdom and Ireland, as well as between India and Pakistan, highlights strong regional and cultural connections (and rivalries).\n",
    "\n",
    "Geopolitical Hotspots: The prominent ranking of pairs like Iran and the United States, and Israel and Palestine (in both directions), strongly indicates that the mappings are successfully capturing communities engaged in intense, real-world geopolitical discourse.\n",
    "\n",
    "Linguistic & Historical Ties: The presence of France/Canada and Brazil/Portugal further validates the mappings by identifying well-known historical and linguistic connections.\n",
    "\n",
    "In conclusion, the fact that the aggregated data mirrors known global relationships so closely serves as a strong validation. It shows the approved mapping list is successfully identifying distinct country-specific communities and capturing their most significant online interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a1ced",
   "metadata": {},
   "source": [
    "# Clusters based on embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accacd9",
   "metadata": {},
   "source": [
    "### 1. Prepare Data\n",
    "First, we'll use a function from our `utilis` module to separate the subreddit names from the embedding vectors and scale the vectors. This scaling step is essential for clustering algorithms to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data, subreddit_labels = ut.prepare_embeddings_for_clustering(df_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ef7d9",
   "metadata": {},
   "source": [
    "### 2. Find Optimal Number of Clusters (k)\n",
    "\n",
    "We can't just guess how many clusters to create. We'll run a K-Means \"Elbow Analysis\" on a sample of the data.\n",
    "\n",
    "**How to Read This Plot:** Look for the \"elbow\" of the armâ€”the point where the line starts to flatten out. This point is usually a good choice for the number of clusters. Adding more clusters after this point gives diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take few minutes to run\n",
    "k_value_list=[10, 50, 100, 150, 200, 250, 300, 500, 700, 800, 1000]\n",
    "\n",
    "elbow_df = ut.calculate_kmeans_elbow_wide(scaled_data, k_value_list, n_samples=5000)\n",
    "\n",
    "pl.plot_kmeans_elbow(elbow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441554b5",
   "metadata": {},
   "source": [
    "### 3. Run Final Clustering & Visualization\n",
    "\n",
    "Now, looking at the elbow plot above we can choose k=500. \n",
    "\n",
    "This function will:\n",
    "1.  Run the final K-Means clustering on the **full 50k+ dataset**.\n",
    "2.  Run **t-SNE**, a powerful algorithm that reduces the 300+ dimensions down to 2D (x, y) so we can plot them.\n",
    "3.  Create an interactive scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHOOSE YOUR PARAMETERS HERE ---\n",
    "N_CLUSTERS = 500  # Your number of clusters\n",
    "MIN_SIZE = 300     # Your minimum size requirement\n",
    "PERPLEXITY = 50   \n",
    "\n",
    "tsne_df, all_cluster_labels = ut.run_clustering_and_tsne(\n",
    "    scaled_data, \n",
    "    subreddit_labels, \n",
    "    n_clusters=N_CLUSTERS,\n",
    "    min_cluster_size=MIN_SIZE,\n",
    "    perplexity=PERPLEXITY  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f92b0",
   "metadata": {},
   "source": [
    "### 4. Manually Label Clusters\n",
    "\n",
    "Now we'll inspect the clusters. The cell below will get a sample of subreddits from all valid clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398759f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_clusters = set(tsne_df['cluster'].astype(int))\n",
    "\n",
    "# Build the full dataframe of all subreddits and their cluster IDs\n",
    "full_df = pd.DataFrame({\n",
    "    'subreddit': subreddit_labels,\n",
    "    'cluster': all_cluster_labels\n",
    "})\n",
    "    \n",
    "# Filter this full dataframe to only include subreddits in valid clusters\n",
    "valid_df = full_df[full_df['cluster'].isin(valid_clusters)]\n",
    "    \n",
    "# Get samples from our valid clusters\n",
    "cluster_samples = ut.get_cluster_samples(\n",
    "    valid_df['subreddit'].values,\n",
    "    valid_df['cluster'].values,\n",
    "    n_samples=100  # Show 100 samples from each\n",
    ")\n",
    "    \n",
    "# Print the samples for inspection\n",
    "for cluster_id, subs in cluster_samples.items():\n",
    "        print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "        print(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    # --- HIGH CLARITY: NSFW / Pornography ---\n",
    "    '35': 'Pornography (General/Hardcore)',\n",
    "    '106': 'Pornography (Fetish/Niche)',\n",
    "    '252': 'Pornography (Specific Niches)',\n",
    "\n",
    "    # --- HIGH CLARITY: Gaming & Fandom ---\n",
    "    '347': 'NSFW (Porn, Selfies, Misc)',\n",
    "    '114': 'Video Gaming (General)',\n",
    "    '327': 'PC Gaming, Hardware & Mods',\n",
    "\n",
    "    # --- HIGH CLARITY: Politics & News ---\n",
    "    '119': 'US Politics (Contentious)',\n",
    "    '419': 'Politics, Science & Social Issues',\n",
    "\n",
    "    # --- HIGH CLARITY: Hobbies / Local / University ---\n",
    "    '85': 'Business, Hobbies & Learning',\n",
    "    '89': 'Local, University & Tech Hobbies',\n",
    "    '391': 'Images & GIFs (SFW)',\n",
    "    '490': 'Local, University & Products',\n",
    "    '454': 'Celebrities, Models & Fashion',\n",
    "    '27': 'Local/Regional & Hobbies',\n",
    "    \n",
    "    # --- MEDIUM CLARITY: Mixed Themes ---\n",
    "    '54': 'Gaming, Fandom & Circlejerks',\n",
    "    '245': 'Politics, Science & Global News',\n",
    "    '187': 'Misc / Gaming, Personals & Trading',\n",
    "    '275': 'General Reddit Interests (SFW)',\n",
    "\n",
    "    # --- LOW CLARITY: \"Junk\" / User-Spam ---\n",
    "    '1': 'Misc / Personals & Trading (Junk)',\n",
    "    '2': 'Misc / Unclear (Junk)',\n",
    "    '20': 'Misc / Unclear (Junk)',\n",
    "    '91': 'Misc / NSFW & User-Spam (Junk)',\n",
    "    '254': 'Misc / User-Spam & NSFW (Junk)',\n",
    "    '335': 'Misc / User-Specific & Bots (Junk)',\n",
    "}\n",
    "\n",
    "# --- Now, we plot using the new labels ---\n",
    "\n",
    "pl.plot_labeled_cluster_map(tsne_df, label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9e844",
   "metadata": {},
   "source": [
    "# Faction Analysis using Embeddings\n",
    "This analysis attempts to find \"strict\" subreddits (those that are semantically closer to their own country's subreddits than to any other) and their closest neighbors based on embedding distance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strict = ut.find_strict_subreddits(df_countries, df_embeddings_countries)\n",
    "print(f\"Found {len(df_strict)} 'strict' subreddits.\")\n",
    "print(\"\\nTop 10 countries by number of 'strict' subreddits:\")\n",
    "display(df_strict.groupby(\"predicted_country\").size().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_df = ut.find_closest_dissimilar_subreddits(df_strict, df_embeddings)\n",
    "print(\"\\nClosest subreddits from different countries (based on embedding distance):\")\n",
    "display(closest_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35dfb8c",
   "metadata": {},
   "source": [
    "As we can see performing the analysis on embedding distance is not leading us to correct results even when considering only subreddit strongly related to countries. The closest subreddit always belong to a country that is not geographical/cultural/political related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf1a6a",
   "metadata": {},
   "source": [
    "# Faction Analysis using Positive Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e6436",
   "metadata": {},
   "source": [
    "### 1. Factions creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51144f66",
   "metadata": {},
   "source": [
    "Let's look at the raw post count originating from each country's subreddits to understand general activity levels. Here we are using the larger list of country mappings that have not been manually approved yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddba7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_activity = ut.calculate_country_activity(df_post_with_1_country, df_countries)\n",
    "print(\"Top 20 most active countries by post volume:\")\n",
    "display(country_activity.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967be20",
   "metadata": {},
   "source": [
    "We can clearly see that there is a big difference in the number of posts associated to each country so it is recommended to normalized every country by the number of posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c49024",
   "metadata": {},
   "source": [
    "This is a more robust analysis. It builds a network graph where countries are nodes. The edge weight is based on the number of positive posts, **normalized by the total activity of each country**. This normalization prevents large countries (like Canada or USA) from dominating all factions simply due to high post volume.\n",
    "\n",
    "This method helps identify strong communities (factions) based on *relative* interaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec296cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "factions_summary_norm, factions_df_norm = ut.detect_normalized_factions(df_post_with_1_country, df_countries)\n",
    "print(\"--- Factions based on Normalized Positive Interactions ---\")\n",
    "display(factions_summary_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e41c0",
   "metadata": {},
   "source": [
    "### 2. Interactive world map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d01659",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot_faction_world_map(factions_df_norm, title=\"World Map of Normalized Factions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af380db",
   "metadata": {},
   "source": [
    "### 3. Missing Countries\n",
    "Some countries may not appear in a faction if they have no positive interactions, only negative ones, or are otherwise disconnected from the main graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ba0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = ut.diagnose_unfactioned_countries(\n",
    "    df_countries, \n",
    "    factions_df_norm, \n",
    "    df_post_with_1_country\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Summary of Unfactioned Countries ---\")\n",
    "display(missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aff411",
   "metadata": {},
   "source": [
    "### 4.Faction Dynamics Over Time\n",
    "Here, we analyze how factions change from quarter to quarter using the source-normalized method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_between_mapped = ut.map_countries_to_posts(df_post_between_countries, df_countries)\n",
    "\n",
    "q_factions_norm_df = ut.analyze_source_normalized_factions_over_time(df_post_between_mapped)\n",
    "print(\"Quarterly Faction Summary (Source-Normalized):\")\n",
    "display(q_factions_norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad40388",
   "metadata": {},
   "source": [
    "Which countries consistently appear in the same faction together over time? A high fraction means a strong, stable bond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9618cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_pairs_norm = ut.find_stable_pairs(q_factions_norm_df)\n",
    "print(\"Most Stable Country Pairs (Source-Normalized):\")\n",
    "display(stable_pairs_norm.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123a0fb",
   "metadata": {},
   "source": [
    "#### Loyalty Score\n",
    "Which countries tend to switch factions most often? A low score means the country frequently changes its primary allies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c959ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loyalty_norm = ut.calculate_loyalty_scores(q_factions_norm_df)\n",
    "print(\"Most 'Loyal' Countries (Most likely to stay with the same allies, Score < 1.0):\")\n",
    "display(loyalty_norm.sort_values(\"loyalty_score\", ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28715c",
   "metadata": {},
   "source": [
    "#### Faction Switching Analysis\n",
    "Can negative posts in a *previous* quarter explain why a country switched factions in the *current* quarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_triggers = ut.find_switch_triggers(q_factions_norm_df, df_post_between_mapped)\n",
    "print(\"Switches potentially triggered by prior negative interactions:\")\n",
    "display(switch_triggers.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f341089",
   "metadata": {},
   "source": [
    "Not enough posts to explain a cause for switching factions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2738af",
   "metadata": {},
   "source": [
    "### 5.Signed Network Visualization\n",
    "Finally, let's visualize the overall network. This graph shows the net sentiment (positive posts - negative posts) between all countries.\n",
    "\n",
    "- **Nodes** are colored by the faction they belong to (from the normalized analysis).\n",
    "- **Green Edges** = Net Positive Interaction\n",
    "- **Red Edges** = Net Negative Interaction\n",
    "- **Edge Width** = Total number of posts (log-scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mapping countries to post data for network plot...\")\n",
    "\n",
    "df_post_1_mapped = ut.map_countries_to_posts(df_post_with_1_country, df_countries)\n",
    "print(\"Mapping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot_signed_network(\n",
    "    df_post_1_mapped, \n",
    "    factions_df_norm, \n",
    "    title=\"Signed Network of Country Interactions (by Faction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1c873",
   "metadata": {},
   "source": [
    "We can clearly see that biggest states are in the middle and are the vertices of many edges reflecting their user's community activity while small states and island are very far and isolated \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1032c",
   "metadata": {},
   "source": [
    "# Global Conditional Probability of Reciprocity\n",
    "\n",
    "In simple terms, we want to answers the question:\n",
    "> \"If Country A posts to Country B, what is the probability that Country B will post back to Country A within 7 days?\"\n",
    "\n",
    "## Function Analysis: `response_global`\n",
    "\n",
    "### 1. Preparation and Filtering\n",
    "\n",
    "* **Clean:** Drops any rows where the source or target country is missing, and converts the `TIMESTAMP` column into a proper datetime format.\n",
    "* **Create `pair_key`:** To analyze a pair like (Italy, France), it needs to group all `Italy -> France` and `France -> Italy` interactions together. It uses a `frozenset` because `frozenset({'Italy', 'France'})` is the *same* as `frozenset({'France', 'Italy'})`. This lets it group all interactions for a pair, regardless of direction.\n",
    "* **Filter `valid_pairs`:** It counts how many interactions each pair has. If a pair only has one post (e.g., Italy posted to France, but France *never* posted), it's impossible to measure reciprocity. So, it only keeps pairs with 2 or more interactions.\n",
    "\n",
    "### 2. The Deterministic Logic \n",
    "\n",
    "* **Loop:** It loops through every `valid_pair` (e.g., the `{'Italy', 'France'}` pair).\n",
    "* **Find the \"True\" Initiator:** For each pair, it gets all their interactions and finds the **very first post** (`first_interaction = df_pair.iloc[0]`).\n",
    "* **Assign Roles:** Based on that *first post*, it permanently assigns the roles:\n",
    "    * `country_A` = The `SOURCE_COUNTRY` of that very first post (The \"Initiator\").\n",
    "    * `country_B` = The `TARGET_COUNTRY` of that very first post (The \"Responder\").\n",
    "* **Separate Data:** It then creates two new DataFrames:\n",
    "    * `df_A_to_B`: All posts where the Initiator (A) posted to the Responder (B).\n",
    "    * `df_B_to_A`: All posts where the Responder (B) posted back to the Initiator (A).\n",
    "\n",
    "### 3. The 7-Day Window (The `merge_asof`)\n",
    "\n",
    "* **`pd.merge_asof`:** This is a powerful time-series function. It takes *every single post* from `df_A_to_B` (an \"initiation\") and searches in `df_B_to_A` to find the **first response** that happened *after* it (`direction='forward'`).\n",
    "* **Calculate `response_time`:** It creates a new column by subtracting the initiation time from the response time.\n",
    "* **Check Window:** It checks if this `response_time` is less than or equal to 7 days.\n",
    "* **Count Totals:**\n",
    "    * `total_initiators_global` is incremented by the total number of initiations (the length of `df_A_to_B`).\n",
    "    * `total_responses_global` is incremented only by the number of responses that were `True` for the 7-day check.\n",
    "\n",
    "### 4. Final Calculation\n",
    "\n",
    "* It divides `total_responses_global` by `total_initiators_global` to get the final conditional probability (e.g., `P(B->A | A->B)`).\n",
    "* It prints this final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.response_global(df_country_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b30c50",
   "metadata": {},
   "source": [
    "# Conditional Probability of Reciprocity within the same country\n",
    "\n",
    "In simple terms, we want to answers the question:\n",
    "> \"If a post from Subreddit A is directed to Subreddit B (both in the same country), what is the probability that Subreddit B will post back to Subreddit A within 7 days?\"\n",
    "\n",
    "## Function Analysis: `response_intra_country`\n",
    "\n",
    "## 1. Preparation and Filtering\n",
    "\n",
    "* **Clean & Filter:** It only keeps rows where `SOURCE_COUNTRY` and `TARGET_COUNTRY` are the same (and not null), AND where `SOURCE_SUBREDDIT` is different from `TARGET_SUBREDDIT`.\n",
    "* **Create `pair_key`:** Just like the global function, it uses a `frozenset` to group subreddit pairs (e.g., `frozenset({'r/news', 'r/politics'})`).\n",
    "* **Filter `valid_pairs`:** It only keeps pairs of subreddits that have interacted 2 or more times.\n",
    "\n",
    "## 2. The Deterministic Logic \n",
    "\n",
    "* **Loop:** It loops through every `valid_pair` (e.g., the `{'r/news', 'r/politics'}` pair).\n",
    "* **Find the \"True\" Initiator:** For each pair, it gets all their interactions (which are already sorted by time) and finds the **very first post** (`first_interaction = df_pair.iloc[0]`).\n",
    "* **Assign Roles:** Based on that *first post*, it permanently assigns the roles:\n",
    "    * `Sub_A` = The `SOURCE_SUBREDDIT` of that first post (The \"Initiator\").\n",
    "    * `Sub_B` = The `TARGET_SUBREDDIT` of that first post (The \"Responder\").\n",
    "* **Separate Data:** It creates two DataFrames:\n",
    "    * `df_A_to_B`: All posts from Subreddit A to Subreddit B.\n",
    "    * `df_B_to_A`: All posts from Subreddit B back to Subreddit A.\n",
    "\n",
    "## 3. The 7-Day Window (The `merge_as_of`)\n",
    "\n",
    "This is where the 7-day probability is calculated.\n",
    "\n",
    "* **`pd.merge_asof`:** It takes *every single post* from `df_A_to_B` (an \"initiation\") and searches in `df_B_to_A` to find the **first response** that happened *after* it (`direction='forward'`).\n",
    "* **Count Totals:** It counts how many initiations (`total_initiators_global`) and how many valid responses within 7 days (`total_responses_global`) were found.\n",
    "\n",
    "## 4. Final Calculation\n",
    "\n",
    "* It divides the total responses by the total initiations to get the final conditional probability.\n",
    "* It prints this final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00af5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.response_intra_country(df_country_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2167b03",
   "metadata": {},
   "source": [
    "# Linguistic Style Mirroring Test\n",
    "\n",
    "We want to answer the question:\n",
    "> \"When a post (A) triggers a reply (B), does the *style* of the reply (B) unconsciously mimic the *style* of the original post (A)?\"\n",
    "\n",
    "To prove this, we can't just measure the similarity of reply pairs. We need to prove that they are *more similar than they would be by pure chance*. This requires a:\n",
    "\n",
    "1.  **Test Group:** A list of similarity scores from *actual* reciprocal pairs (A replied to B).\n",
    "2.  **Control Group:** A list of similarity scores from *randomly selected* pairs (X and Y, who have no connection).\n",
    "3.  **Comparison:** A statistical test (like a T-test) to see if the Test Group's similarity is significantly higher than the Control Group's.\n",
    "\n",
    "This experimental design is complex. To keep the code clean and readable, we split the logic:\n",
    "\n",
    "* **`find_reciprocity_pairs_and_similarity()` (The \"Helper\" Function):** This function has one very specific and difficult job: to find every valid `A -> B => B -> A` interaction and return the list of their similarity scores. This will be our **Test Group**.\n",
    "* **`response_similarity()` (The \"Main\" Function):** This function acts as the \"orchestrator.\" It defines the experiment, calls the helper function to get the Test Group, creates the **Control Group** itself, and then performs the final statistical comparison and visualization.\n",
    "\n",
    "---\n",
    "## The Helper Function: `find_reciprocity_pairs_and_similarity`\n",
    "\n",
    "This function builds our **Test Group**.\n",
    "\n",
    "1.  **Define Triggers (A) and Responses (B):**\n",
    "    * It loads the `matches_csv` to get a list of \"country subreddits.\"\n",
    "    * A **\"Trigger\" (A)** is defined as any *non-country* subreddit posting *to* a *country* subreddit.\n",
    "    * A **\"Response\" (B)** is defined as a *country* subreddit posting *to* a *non-country* subreddit.\n",
    "\n",
    "2.  **Index Responses (An Optimization):**\n",
    "    * Instead of re-scanning the entire list of responses for every single trigger, it creates a `response_lookup` dictionary. This index allows it to instantly find all potential responses for a given pair (e.g., all posts from `'r/italy'` to `'r/formula1'`).\n",
    "\n",
    "3.  **Iterate and Find Pairs:**\n",
    "    * It loops through *every single Trigger (A)* post.\n",
    "    * For each trigger (e.g., `r/formula1 -> r/italy`), it calculates the 7-day window *from that post's timestamp*.\n",
    "    * It uses the `response_lookup` to find all matching Responses (B) (e.g., `r/italy -> r/formula1`) that fall *within that 7-day window*.\n",
    "\n",
    "4.  **Calculate Similarity:**\n",
    "    * If valid responses are found, it takes the **first** one (`.iloc[0]`).\n",
    "    * It gets the \"style vector\" (the list of 12 LIWC/VADER features) for the trigger post (A) and the response post (B).\n",
    "    * It calculates the **cosine similarity** between these two vectors.\n",
    "    * This similarity score is added to the `similarity_scores` list.\n",
    "\n",
    "This function returns the complete `similarity_scores` list, which is our **Test Group**.\n",
    "\n",
    "---\n",
    "## The Main Function: `response_similarity`\n",
    "\n",
    "This function takes the Test Group from the helper and runs the full experiment.\n",
    "\n",
    "1.  **Define Style Vector:** It defines `style_features_list`. This is a crucial choice: we are *only* testing style (tone, pronouns, etc.), not the *topic* (work, money, etc.).\n",
    "2.  **Prepare Data:** Filters it down to only the columns needed for the style vectors.\n",
    "3.  **Get Test Group:** It calls `find_reciprocity_pairs_and_similarity()` to get the list of scores (`reciprocity_similarities`).\n",
    "4.  **Create Control Group:**\n",
    "    * It counts how many scores are in the Test Group (e.g., `N = 1,500`).\n",
    "    * It then creates a `baseline_similarities` list by **randomly sampling** `N` pairs of posts from the *entire dataset* and calculating their similarity. This list represents the similarity that occurs \"by pure chance.\"\n",
    "5.  **Run Statistical Tests:**\n",
    "    * It compares the two lists (`Reciprocal` vs. `Random`) using a T-test and a Mann-Whitney U-test.\n",
    "    * It specifically uses `alternative='greater'` to test our hypothesis: \"Is the Reciprocal group's similarity *greater than* the Random group's?\"\n",
    "6.  **Interpret Results & Plot:**\n",
    "    * It checks the **p-value**. A `p < 0.05` is considered a **significant result**, meaning the similarity we see in the Test Group is not just a random fluke. This provides evidence for our \"Stylistic Mirroring\" hypothesis.\n",
    "    * It then plots both distributions on a KDE plot, allowing us to *visually* confirm if the Reciprocal group's curve is shifted to the right (more similar) than the Random group's curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.response_similarity(df_country_exp, MAP_COUNTRY_EXPANDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eec419",
   "metadata": {},
   "source": [
    "# Political analysis\n",
    "\n",
    "This analysis identifies interactions between a country's subreddit and a subreddit associated with a specific political ideology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579496ad",
   "metadata": {},
   "source": [
    "### 1. Load Political Analysis Data\n",
    "\n",
    "We'll load the political subreddits obteined with `filter_politic_subreddits.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLITIC_FILE=\"data/politic_subreddit.csv\"\n",
    "df_politic=pd.read_csv(POLITIC_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202b25a",
   "metadata": {},
   "source": [
    "### 2. Analyze activity by political subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9855ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_results = ut.analyze_political_activity(df_posts, df_politic)\n",
    "\n",
    "print(\"\\n--- Political Activity Summary ---\")\n",
    "print(f\"Total Posts Involving Political Subreddits: {activity_results['total_political_posts']:,}\")\n",
    "    \n",
    "print(\"\\nMost Active Subreddit:\")\n",
    "print(f\"  Name:  {activity_results['most_active_subreddit']['name']}\")\n",
    "print(f\"  Posts: {activity_results['most_active_subreddit']['posts']:,}\")\n",
    "\n",
    "print(\"\\nMost Active Ideology:\")\n",
    "print(f\"  Name:  {activity_results['most_active_ideology']['name']}\")\n",
    "print(f\"  Posts: {activity_results['most_active_ideology']['posts']:,}\")\n",
    "    \n",
    "print(\"\\n--- Top 10 Most Active Ideologies ---\")\n",
    "display(activity_results['ideology_activity_summary'].head(10))\n",
    "    \n",
    "print(\"\\n--- Top 10 Most Active Subreddits ---\")\n",
    "display(activity_results['subreddit_activity_summary'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281cd4c",
   "metadata": {},
   "source": [
    "### 3. Compute interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pure_summary_2015 = ut.analyze_pure_cross_interactions(\n",
    "    df_posts,   \n",
    "    df_countries,   \n",
    "    df_politic\n",
    ")\n",
    "\n",
    "display(df_pure_summary_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e0c6f",
   "metadata": {},
   "source": [
    "We can say that although right subreddits tend to post more they don't have many interactions with country related subreddits. Instead we can clearly see that there is a surprising number of interactions between centre subreddits wihich put in evudence how this ideology is more inclined to the positive discussion and debate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886c3a",
   "metadata": {},
   "source": [
    "# Sport Analysis\n",
    "\n",
    "This analysis answers the question: **\"Which single sport is the most popular (has the most interactions) in each country?\"**\n",
    "\n",
    "### 1. Load Sport Analysis Data\n",
    "\n",
    "We'll load the (`df_country_sports`) obteined with `filter_sports.py`. \n",
    "\n",
    "This file is special: an interaction can be `Country -> Sport` or `Sport -> Country`. This means a single row might have a `NaN` value in either `SOURCE_COUNTRY` or `TARGET_COUNTRY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73866924",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPORT_FILE = str(\"data/df_country_sport.csv\")\n",
    "df_analysis = pd.read_csv(SPORT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e5bbe",
   "metadata": {},
   "source": [
    "### 2. Clean the dataset \n",
    "\n",
    "Create one 'Country' column\n",
    "It takes the value from 'SOURCE_COUNTRY'. If that is NaN, it fills it with the value from 'TARGET_COUNTRY'.\n",
    "Same with 'Sport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['Country'] = df_analysis['SOURCE_COUNTRY'].fillna(df_analysis['TARGET_COUNTRY'])\n",
    "\n",
    "df_analysis['Sport'] = df_analysis['SPORT_SOURCE'].fillna(df_analysis['SPORT_TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34f041",
   "metadata": {},
   "source": [
    "### 3. Agregate interactions \n",
    "\n",
    "We group by the new clean columns and count the occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_interactions = df_analysis.groupby(['Country', 'Sport']).size().reset_index(name='total_interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b83422",
   "metadata": {},
   "source": [
    "### 4. Find the Top Sport for Each Country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7307cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index (row number) of the maximum interaction count *within* each country group\n",
    "idx_of_max_per_group = agg_interactions.groupby('Country')['total_interactions'].idxmax()\n",
    "\n",
    "# Select only those rows using .loc[]\n",
    "df_top_sport_per_country = agg_interactions.loc[idx_of_max_per_group]\n",
    "\n",
    "# Sort the final list by interaction count for readability\n",
    "df_top_sport_per_country = df_top_sport_per_country.sort_values('total_interactions', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top Sport per Country (Ranked by Interaction Count) ---\")\n",
    "print(df_top_sport_per_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204f72e",
   "metadata": {},
   "source": [
    "# Connectivity Analysis\n",
    "\n",
    "This analysis answers the question: **\"How are the countries connected in the Reddit space via Hyperlinks and how easy it is to travel from one country to another\"**\n",
    "\n",
    "### 1. Load the necessary data\n",
    "\n",
    "We generate a gpickle file to set up a graph and load the countries found into 'COUNTRY_CSV_FILENAME' and generate a new file for our shortest paths 'COUNTRY_PATHS_OUTPUT'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca60289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants for network analysis ---\n",
    "# Path for the saved/loaded pre-computed graph\n",
    "PATH = \"data/\"\n",
    "FILENAME_TITLES = 'soc-redditHyperlinks-title.tsv'\n",
    "FILENAME_BODIES = 'soc-redditHyperlinks-body.tsv'\n",
    "GRAPH_FILENAME = os.path.join(PATH, \"subreddit_graph.gpickle\")\n",
    "\n",
    "# Path for the country mapping file\n",
    "COUNTRY_CSV_FILENAME = os.path.join(PATH, \"subreddit_matches_approved.csv\")\n",
    "\n",
    "# Path for the final output file\n",
    "COUNTRY_PATHS_OUTPUT = os.path.join(PATH, \"country_shortest_paths_output.csv\")\n",
    "\n",
    "df_title, df_body = na.load_dataframes(PATH, FILENAME_TITLES, FILENAME_BODIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dca6f3",
   "metadata": {},
   "source": [
    "Get a quick sanity check on the data we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_title is not None and df_body is not None:\n",
    "    print(\"--- Titles DataFrame (Raw) ---\")\n",
    "    display(df_title.head())\n",
    "    print(\"\\n--- Bodies DataFrame (Raw) ---\")\n",
    "    display(df_body.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dc751",
   "metadata": {},
   "source": [
    "### 2. Build the graph from scratch or use the one that exists already\n",
    "This uses the 'df_title' DataFrame from earlier. It will load 'subreddit_graph.gpickle' if it exists, or build it from the DataFrame if it doesn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na.remove_saved_graph(GRAPH_FILENAME)\n",
    "\n",
    "graph, source_subreddits, all_shortest_paths = None, None, None\n",
    "if df_title is not None:\n",
    "    graph, source_subreddits, all_shortest_paths = na.load_or_build_graph_data(df_title, GRAPH_FILENAME)\n",
    "else:\n",
    "    print(\"df_title not loaded, cannot build graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f6871",
   "metadata": {},
   "source": [
    "### 3. Plot the countries network \n",
    "This builds a new graph of countries and plots it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph:\n",
    "    na.plot_country_graph(graph, COUNTRY_CSV_FILENAME)\n",
    "else:\n",
    "    print(\"Graph not loaded, cannot plot country graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079a159",
   "metadata": {},
   "source": [
    "### 4. Calculate shortest paths from country to country\n",
    "The shortest path is calculated using the number of hyperlinks between two subreddits as an inverse edge weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ac8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate and save country-to-country shortest paths ---\n",
    "\n",
    "if all_shortest_paths:\n",
    "    na.calculate_country_shortest_paths(all_shortest_paths, COUNTRY_CSV_FILENAME, COUNTRY_PATHS_OUTPUT)\n",
    "else:\n",
    "    print(\"Shortest paths not available, cannot calculate country paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97596d7a",
   "metadata": {},
   "source": [
    "### 5. Show the final paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the results from the saved CSV\n",
    "try:\n",
    "    df_country_paths = pd.read_csv(COUNTRY_PATHS_OUTPUT)\n",
    "    df_relevant_rows = df_country_paths[\n",
    "        (df_country_paths['source_country'] != df_country_paths['target_country']) & (df_country_paths['shortest_path_length'] < np.inf)\n",
    "    ]\n",
    "    print(\"\\n--- Shortest Paths Between Countries (Top 10) ---\")\n",
    "    display(df_relevant_rows.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not find results file at {COUNTRY_PATHS_OUTPUT}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc0139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ada-project)",
   "language": "python",
   "name": "ada-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
